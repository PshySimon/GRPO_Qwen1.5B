"""æµ‹è¯•å¾®è°ƒåçš„æ¨¡å‹ - æ”¯æŒæŒ‡å®šcheckpoint"""

import argparse
import os
from pathlib import Path

import torch
import yaml
from src.data.utils import SYSTEM_PROMPT, build_prompt, extract_answer_from_model_output
from src.utils.device import get_device
from src.utils.logging import setup_logger
from transformers import AutoModelForCausalLM, AutoTokenizer


def find_latest_experiment():
    """æ‰¾åˆ°æœ€æ–°çš„å®éªŒç›®å½•"""
    experiments_dir = Path("output/experiments")
    if not experiments_dir.exists():
        return None

    experiment_dirs = [
        d for d in experiments_dir.iterdir() if d.is_dir() and d.name.startswith("exp_")
    ]
    if not experiment_dirs:
        return None

    # æŒ‰æ—¶é—´æˆ³æ’åºï¼Œè¿”å›æœ€æ–°çš„
    latest = max(experiment_dirs, key=lambda x: x.name)
    return str(latest)


def parse_checkpoint_step(checkpoint_name):
    """ä»checkpointåç§°ä¸­æå–stepæ•°å­—ç”¨äºæ’åº"""
    import re

    # åŒ¹é… iteration_X_step_Y æ ¼å¼
    match = re.search(r"iteration_(\d+)_step_(\d+)", checkpoint_name)
    if match:
        iteration = int(match.group(1))
        step = int(match.group(2))
        return iteration, step
    return 0, 0


def find_model_path(experiment_dir):
    """åœ¨å®éªŒç›®å½•ä¸­æ‰¾åˆ°æ¨¡å‹è·¯å¾„"""
    experiment_path = Path(experiment_dir)

    # ä¼˜å…ˆä½¿ç”¨final_model
    final_model_path = experiment_path / "final_model"
    if final_model_path.exists():
        return str(final_model_path)

    # å¦‚æœæ²¡æœ‰final_modelï¼Œä½¿ç”¨æœ€æ–°çš„checkpoint
    checkpoints_dir = experiment_path / "checkpoints"
    if checkpoints_dir.exists():
        checkpoints = [d for d in checkpoints_dir.iterdir() if d.is_dir()]
        if checkpoints:
            # æŒ‰iterationå’Œstepæ•°å­—æ’åºï¼Œæ‰¾æœ€æ–°çš„
            latest_checkpoint = max(
                checkpoints, key=lambda x: parse_checkpoint_step(x.name)
            )
            return str(latest_checkpoint)

    return None


def list_checkpoints(experiment_dir):
    """åˆ—å‡ºå®éªŒç›®å½•ä¸­çš„æ‰€æœ‰checkpoint"""
    experiment_path = Path(experiment_dir)
    checkpoints_dir = experiment_path / "checkpoints"

    if not checkpoints_dir.exists():
        return []

    checkpoints = [d for d in checkpoints_dir.iterdir() if d.is_dir()]
    # æŒ‰iterationå’Œstepæ•°å­—æ’åº
    sorted_checkpoints = sorted(
        checkpoints, key=lambda x: parse_checkpoint_step(x.name)
    )
    return [str(d) for d in sorted_checkpoints]


def load_resume_config():
    """åŠ è½½æ–­ç‚¹ç»­è®­é…ç½®"""
    resume_file = "config/resume.yaml"
    if os.path.exists(resume_file):
        with open(resume_file, "r") as f:
            return yaml.safe_load(f)
    return None


def test_model_with_path(model_path, logger):
    """ä½¿ç”¨æŒ‡å®šè·¯å¾„æµ‹è¯•æ¨¡å‹"""
    device = get_device()

    if not os.path.exists(model_path):
        logger.error(f"æ¨¡å‹è·¯å¾„ä¸å­˜åœ¨: {model_path}")
        return False

    # åŠ è½½æ¨¡å‹
    try:
        logger.info(f"æ­£åœ¨åŠ è½½æ¨¡å‹: {model_path}")
        loaded_model = AutoModelForCausalLM.from_pretrained(
            model_path,
            torch_dtype=torch.float16,
        ).to(device)

        loaded_tokenizer = AutoTokenizer.from_pretrained(
            model_path, padding_side="left"
        )
        logger.info("æ¨¡å‹åŠ è½½æˆåŠŸ")
    except Exception as e:
        logger.error(f"æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        return False

    logger.info("\nğŸ¤– GRPOæ•°å­¦é—®é¢˜æ±‚è§£å™¨ - äº¤äº’å¼å¯¹è¯æ¨¡å¼")
    logger.info("=" * 60)
    logger.info("è¾“å…¥æ•°å­¦é—®é¢˜ï¼Œæ¨¡å‹ä¼šä¸ºä½ æ±‚è§£ï¼")
    logger.info("è¾“å…¥ 'quit', 'exit', 'q' é€€å‡ºç¨‹åº")
    logger.info("=" * 60)

    while True:
        try:
            # è·å–ç”¨æˆ·è¾“å…¥
            user_input = input("\nğŸ§® è¯·è¾“å…¥æ•°å­¦é—®é¢˜: ").strip()

            # æ£€æŸ¥é€€å‡ºå‘½ä»¤
            if user_input.lower() in ["quit", "exit", "q", ""]:
                print("ğŸ‘‹ å†è§ï¼")
                break

            if not user_input:
                continue

            print(f"\nğŸ“ é—®é¢˜: {user_input}")
            print("ğŸ¤– å›ç­”: ", end="", flush=True)

            # å‡†å¤‡æç¤º
            test_messages = [
                {"role": "system", "content": SYSTEM_PROMPT},
                {"role": "user", "content": user_input},
            ]
            test_prompt = build_prompt(test_messages)

            # ç”Ÿæˆå“åº” - æµå¼è¾“å‡º
            test_input_ids = loaded_tokenizer.encode(
                test_prompt, return_tensors="pt", padding=True, padding_side="left"
            ).to(device)

            # ä½¿ç”¨æ ‡å‡†generateæ–¹æ³•ï¼ˆç¡®ä¿æ­£ç¡®æ€§ï¼‰
            with torch.no_grad():
                test_output_ids = loaded_model.generate(
                    test_input_ids,
                    max_new_tokens=400,
                    num_return_sequences=1,
                    pad_token_id=loaded_tokenizer.pad_token_id,
                    eos_token_id=loaded_tokenizer.eos_token_id,
                    do_sample=False,  # ç¡®å®šæ€§ç”Ÿæˆ
                    early_stopping=False,
                    # è¦†ç›–generation_configä¸­çš„é‡‡æ ·å‚æ•°
                    temperature=1.0,
                    top_p=1.0,
                    top_k=0,
                )

            # è·å–å®Œæ•´å“åº”
            test_response = loaded_tokenizer.decode(
                test_output_ids[0], skip_special_tokens=True
            )

            # æå–æ–°ç”Ÿæˆçš„éƒ¨åˆ†ï¼ˆå»æ‰promptï¼‰
            original_prompt = loaded_tokenizer.decode(
                test_input_ids[0], skip_special_tokens=True
            )
            if test_response.startswith(original_prompt):
                generated_part = test_response[len(original_prompt) :].strip()
            else:
                generated_part = test_response

            # æµå¼æ˜¾ç¤ºæ•ˆæœï¼ˆæ¨¡æ‹Ÿï¼‰
            import time

            for char in generated_part:
                print(char, end="", flush=True)
                time.sleep(0.01)  # æ‰“å­—æœºæ•ˆæœ

            print()  # æ¢è¡Œ

            # æå–æœ€ç»ˆç­”æ¡ˆ
            try:
                extracted_answer = extract_answer_from_model_output(generated_part)
                if extracted_answer:
                    print(f"\nğŸ’¡ æå–çš„ç­”æ¡ˆ: {extracted_answer}")
                else:
                    print("\nâš ï¸  æ— æ³•ä»å›ç­”ä¸­æå–æ˜ç¡®ç­”æ¡ˆ")

            except Exception as e:
                print(f"\nâŒ ç­”æ¡ˆæå–å¤±è´¥: {e}")

        except KeyboardInterrupt:
            print("\n\nğŸ‘‹ ç¨‹åºè¢«ä¸­æ–­ï¼Œå†è§ï¼")
            break
        except Exception as e:
            print(f"\nâŒ å‘ç”Ÿé”™è¯¯: {e}")
            print("è¯·é‡è¯•æˆ–è¾“å…¥ 'quit' é€€å‡º")

    logger.info("\næµ‹è¯•ä¼šè¯ç»“æŸï¼")
    return True


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="æµ‹è¯•å¾®è°ƒåçš„GRPOæ¨¡å‹")
    parser.add_argument("--checkpoint", type=str, help="æŒ‡å®šcheckpointè·¯å¾„")
    parser.add_argument("--experiment", type=str, help="æŒ‡å®šå®éªŒç›®å½•")
    parser.add_argument("--list", action="store_true", help="åˆ—å‡ºå¯ç”¨çš„checkpoint")

    args = parser.parse_args()

    logger = setup_logger()

    # å¦‚æœæŒ‡å®šäº†checkpointè·¯å¾„ï¼Œç›´æ¥æµ‹è¯•
    if args.checkpoint:
        logger.info(f"ä½¿ç”¨æŒ‡å®šçš„checkpoint: {args.checkpoint}")
        test_model_with_path(args.checkpoint, logger)
        return

    # å¦‚æœæŒ‡å®šäº†å®éªŒç›®å½•
    if args.experiment:
        if args.list:
            # åˆ—å‡ºè¯¥å®éªŒçš„æ‰€æœ‰checkpoint
            checkpoints = list_checkpoints(args.experiment)
            if checkpoints:
                logger.info(f"å®éªŒ {args.experiment} ä¸­çš„checkpoint:")
                for i, cp in enumerate(checkpoints, 1):
                    logger.info(f"  {i}. {cp}")
                logger.info(
                    "\nä½¿ç”¨æ–¹æ³•: python scripts/test_model.py --checkpoint <è·¯å¾„>"
                )
            else:
                logger.info(f"å®éªŒ {args.experiment} ä¸­æ²¡æœ‰æ‰¾åˆ°checkpoint")
            return
        else:
            # ä½¿ç”¨è¯¥å®éªŒçš„æœ€ä½³æ¨¡å‹
            model_path = find_model_path(args.experiment)
            if model_path:
                logger.info(f"ä½¿ç”¨å®éªŒ {args.experiment} çš„æ¨¡å‹: {model_path}")
                test_model_with_path(model_path, logger)
            else:
                logger.error(f"åœ¨å®éªŒ {args.experiment} ä¸­æœªæ‰¾åˆ°æ¨¡å‹")
            return

    # å¦‚æœåªæ˜¯åˆ—å‡ºï¼Œæ‰¾æœ€æ–°å®éªŒ
    if args.list:
        latest_exp = find_latest_experiment()
        if latest_exp:
            logger.info(f"æœ€æ–°å®éªŒ: {latest_exp}")
            checkpoints = list_checkpoints(latest_exp)
            if checkpoints:
                logger.info("å¯ç”¨çš„checkpoint:")
                for i, cp in enumerate(checkpoints, 1):
                    logger.info(f"  {i}. {cp}")
                logger.info(
                    "\nä½¿ç”¨æ–¹æ³•: python scripts/test_model.py --checkpoint <è·¯å¾„>"
                )
            else:
                logger.info("æ²¡æœ‰æ‰¾åˆ°checkpoint")
        else:
            logger.info("æ²¡æœ‰æ‰¾åˆ°ä»»ä½•å®éªŒ")
        return

    # é»˜è®¤è¡Œä¸ºï¼šè‡ªåŠ¨æ‰¾æ¨¡å‹æµ‹è¯•
    model_path = None

    # å°è¯•ä»resumeé…ç½®è·å–
    resume_config = load_resume_config()
    if resume_config:
        experiment_dir = resume_config["experiment_dir"]
        model_path = find_model_path(experiment_dir)
        if model_path:
            logger.info(f"ä»resumeé…ç½®æ‰¾åˆ°æ¨¡å‹: {model_path}")
        else:
            logger.error(f"åœ¨å®éªŒç›®å½•ä¸­æœªæ‰¾åˆ°æ¨¡å‹: {experiment_dir}")
            return
    else:
        # å°è¯•æ‰¾åˆ°æœ€æ–°å®éªŒ
        latest_exp = find_latest_experiment()
        if latest_exp:
            model_path = find_model_path(latest_exp)
            if model_path:
                logger.info(f"æ‰¾åˆ°æœ€æ–°å®éªŒæ¨¡å‹: {model_path}")
            else:
                logger.error(f"åœ¨æœ€æ–°å®éªŒä¸­æœªæ‰¾åˆ°æ¨¡å‹: {latest_exp}")
                return
        else:
            # å›é€€åˆ°æ—§è·¯å¾„
            model_path = "grpo_finetuned_model"
            if not os.path.exists(model_path):
                logger.error("æœªæ‰¾åˆ°ä»»ä½•å¾®è°ƒæ¨¡å‹ï¼")
                logger.info("ä½¿ç”¨ --list æŸ¥çœ‹å¯ç”¨çš„checkpoint")
                return
            logger.info(f"ä½¿ç”¨æ—§è·¯å¾„æ¨¡å‹: {model_path}")

    if model_path:
        test_model_with_path(model_path, logger)


if __name__ == "__main__":
    main()
